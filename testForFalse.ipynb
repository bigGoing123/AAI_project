{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-04T10:44:56.771245700Z",
     "start_time": "2024-01-04T10:44:52.555118500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from CNN import CNN\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from trainDataSet import trainDataSet\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "torch.manual_seed(1)  # 使用随机化种子使神经网络的初始化每次都相同\n",
    "from testDataSet import testDataSet\n",
    "# 超参数\n",
    "EPOCH = 5 # 训练整批数据的次数\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001  # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# correct_file_label=[]\n",
    "def delete_error_img():\n",
    "    \"\"\"\n",
    "    由于train数据集中图片噪音过多，现用官方数据训练完成的模型对一些标签错误的图片进行筛选，模型准确率已经在98%左右；\n",
    "    如果模型预测的标签与实际标签不一致，则删除该图片\n",
    "    \"\"\"\n",
    "\n",
    "    root_dir = './processed_data/train'  # 替换为数据集根目录路径\n",
    "    dataset = trainDataSet(root_dir)\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    # 加载模型\n",
    "\n",
    "    cnn = CNN()\n",
    "    # 如果模型已经训练过，确保加载模型权重\n",
    "    #该模型是用mnist官方数据集训练完成的\n",
    "    cnn.load_state_dict(torch.load('cnn2.pkl'))\n",
    "    # 将模型设置为评估模式\n",
    "    cnn.eval()\n",
    "    correct_index=[]#这是正确的绝对index\n",
    "    for index, (data, label) in enumerate(tqdm(train_loader)):\n",
    "        # 模型预测\n",
    "        output = cnn(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        # 检查预测是否正确\n",
    "        batch_start_index = index * train_loader.batch_size\n",
    "        for idx, pred in enumerate(predicted):\n",
    "            absolute_idx = batch_start_index + idx  # 计算在整个数据集中的索引\n",
    "            if pred.item() == label[idx].item():\n",
    "                correct_index.append(absolute_idx)\n",
    "                # correct_file_label.append(pred.item())\n",
    "              \n",
    "    print(\"删除噪音数据中........大概2min\")\n",
    "    dataset.get_correct_data(correct_index)\n",
    "    # dataset.set_labels(correct_file_label)\n",
    "    # 创建一个新的 dataset\n",
    "    new_dataset = train_loader.dataset\n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "def train_for_new_model(train_loader):\n",
    "    \"\"\"\n",
    "    dataloader是要训练的数据集\n",
    "    通过cnn训练已经清理完成的数据集，来得到一个针对于该数据集的新模型。\n",
    "    \"\"\"\n",
    "    cnn2 = CNN()\n",
    "    # 优化器选择Adam\n",
    "    optimizer = torch.optim.Adam(cnn2.parameters(), lr=LR)\n",
    "    # 损失函数\n",
    "    loss_func = nn.CrossEntropyLoss()  # 目标标签是one-hotted\n",
    "    # 开始训练\n",
    "    for epoch in range(EPOCH):\n",
    "        for step, (b_x, b_y) in enumerate(tqdm(train_loader)):  # 分配batch data\n",
    "            output = cnn2(b_x)  # 先将数据放到cnn中计算output\n",
    "            loss = loss_func(output, b_y)  # 输出和真实标签的loss，二者位置不可颠倒\n",
    "            optimizer.zero_grad()  # 清除之前学到的梯度的参数\n",
    "            loss.backward()  # 反向传播，计算梯度\n",
    "            optimizer.step()  # 应用梯度\n",
    "    torch.save(cnn2.state_dict(), 'cnnLast.pkl')#保存新模型"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T10:44:56.787943900Z",
     "start_time": "2024-01-04T10:44:56.781956200Z"
    }
   },
   "id": "f15a81212f41da47"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:27<00:00, 34.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除噪音数据中........大概2min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "newDataSet=delete_error_img()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T10:45:51.296377700Z",
     "start_time": "2024-01-04T10:44:56.787943900Z"
    }
   },
   "id": "b50bc1d27b842792"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(newDataSet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T10:32:15.159604800Z",
     "start_time": "2024-01-04T10:32:15.146891Z"
    }
   },
   "id": "a407831a5db90dfc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# newDataSet.set_labels(correct_file_label)\n",
    "filename=newDataSet.data_files\n",
    "labels=newDataSet.labels\n",
    "correctFile=pd.DataFrame({\"name\":filename,\"labels\":labels}).to_csv(\"1.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T10:45:59.412540200Z",
     "start_time": "2024-01-04T10:45:59.278630300Z"
    }
   },
   "id": "6c71f5aef285e9a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# root_dir = './processed_data/train'  # 替换为数据集根目录路径\n",
    "# dataset1 = trainDataSet(root_dir)\n",
    "# train_loader = DataLoader(dataset1, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# print(len(dataset1))\n",
    "# old_filename=train_loader.dataset.data_files\n",
    "# old_labels=train_loader.dataset.labels\n",
    "# pd.DataFrame({\"name\":old_filename,\"labels\":old_labels}).to_csv(\"hahah1.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-04T10:32:15.152605300Z"
    }
   },
   "id": "5187420d533cfb6a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "CNN(\n  (conv1): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Dropout(p=0.5, inplace=False)\n  )\n  (out): Linear(in_features=1568, out_features=10, bias=True)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "# 如果模型已经训练过，确保加载模型权重\n",
    "#该模型是用mnist官方数据集训练完成的\n",
    "cnn.load_state_dict(torch.load('cnn2.pkl'))\n",
    "# 将模型设置为评估模式\n",
    "cnn.eval()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T10:46:33.485560600Z",
     "start_time": "2024-01-04T10:46:33.436299800Z"
    }
   },
   "id": "44ef01afb92dc761"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型在数据集上的准确率：0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#看留下的数据集在原来的模型中准确率如何\n",
    "from PIL import Image\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "root_dir = './processed_data/val'  # 替换为数据集根目录路径\n",
    "valSet = trainDataSet(root_dir)\n",
    "val_loader = DataLoader(valSet, batch_size=BATCH_SIZE, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    # k=0\n",
    "    for index,(data, labels) in enumerate(tqdm(val_loader)):\n",
    "        outputs = cnn(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        image = Image.fromarray(data.numpy().astype('uint8'), 'L')\n",
    "        image.save('1.png')  # 图片将保存在 img 文件夹下\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f'模型在数据集上的准确率：{accuracy * 100:.2f}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T14:53:51.618848400Z",
     "start_time": "2024-01-04T14:53:50.890725700Z"
    }
   },
   "id": "3655c4e22d9a4706"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:05<00:00, 27.55it/s]\n"
     ]
    }
   ],
   "source": [
    "cnn2 = CNN()\n",
    "# 如果模型已经训练过，确保加载模型权重\n",
    "cnn2.load_state_dict(torch.load('cnn2.pkl'))\n",
    "# 将模型设置为评估模式\n",
    "cnn2.eval()\n",
    "#将一个文件夹中的所有文件名写入到一个numpy数组中\n",
    "folder_path = './processed_data/test/'\n",
    "# target_folder = './png_images/new/'\n",
    "test_dataset = testDataSet(folder_path)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "results =[]\n",
    "with torch.no_grad():\n",
    "    for index,data in enumerate(tqdm(test_loader)):\n",
    "        # 获取预测结果\n",
    "        outputs = cnn2(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        results.extend(predicted.cpu().numpy())\n",
    "# 处理或保存测试结果\n",
    "file_names=np.array([str(f)+\".npy\" for f in range(9900)])\n",
    "predictions=pd.DataFrame({'fileNames':file_names,'predication':results})\n",
    "predictions.to_csv('predictions.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T11:08:17.905770600Z",
     "start_time": "2024-01-04T11:08:12.135333400Z"
    }
   },
   "id": "729f20b20dbdabcb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in tqdm(train_loader):\n",
    "        outputs = cnn(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f'模型在数据集上的准确率：{accuracy * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-04T10:32:15.161608700Z"
    }
   },
   "id": "13934bbd17b05bc3"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def delete_error_img(root_dir):\n",
    "    \"\"\"\n",
    "    由于train数据集中图片噪音过多，现用官方数据训练完成的模型对一些标签错误的图片进行筛选，模型准确率已经在98%左右；\n",
    "    如果模型预测的标签与实际标签不一致，则删除该图片\n",
    "    \"\"\"\n",
    "\n",
    "    # root_dir = './processed_data/train'  # 替换为数据集根目录路径\n",
    "    dataset = trainDataSet(root_dir)\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    # 加载模型\n",
    "\n",
    "    cnn = CNN()\n",
    "    # 如果模型已经训练过，确保加载模型权重\n",
    "    # 该模型是用mnist官方数据集训练完成的\n",
    "    cnn.load_state_dict(torch.load('cnn2.pkl'))\n",
    "    # 将模型设置为评估模式\n",
    "    cnn.eval()\n",
    "    correct_index = []  # 这是正确的绝对index\n",
    "    for index, (data, label) in enumerate(tqdm(train_loader)):\n",
    "        # 模型预测\n",
    "        output = cnn(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        # 检查预测是否正确\n",
    "        batch_start_index = index * train_loader.batch_size\n",
    "        for idx, pred in enumerate(predicted):\n",
    "            absolute_idx = batch_start_index + idx  # 计算在整个数据集中的索引\n",
    "            if pred.item() == label[idx].item():\n",
    "                correct_index.append(absolute_idx)\n",
    "                # correct_file_label.append(pred.item())\n",
    "\n",
    "    print(\"删除噪音数据中........大概2min\")\n",
    "    dataset.get_correct_data(correct_index)\n",
    "    # dataset.set_labels(correct_file_label)\n",
    "    # 创建一个新的 dataset\n",
    "    new_dataset = train_loader.dataset\n",
    "    return new_dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:24:49.803964400Z",
     "start_time": "2024-01-04T15:24:49.778893300Z"
    }
   },
   "id": "4b136e5917bd57cb"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "44702"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:28:16.967487600Z",
     "start_time": "2024-01-04T15:28:16.947490400Z"
    }
   },
   "id": "2506d97e77e47172"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 33.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除噪音数据中........大概2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root_dir = './processed_data/val'  # 替换为数据集根目录路径\n",
    "val_set=delete_error_img(root_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:28:59.266263300Z",
     "start_time": "2024-01-04T15:28:59.180982800Z"
    }
   },
   "id": "ab0e50a93c24d398"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "77"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:29:02.085649200Z",
     "start_time": "2024-01-04T15:29:02.062956900Z"
    }
   },
   "id": "848c8e0d017cd9eb"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 26.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型在val集上的准确率：100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "cnn3 = CNN()\n",
    "# 如果模型已经训练过，确保加载模型权重\n",
    "cnn3.load_state_dict(torch.load('cnnLast.pkl'))\n",
    "# 将模型设置为评估模式\n",
    "cnn3.eval()\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for data, labels in tqdm(val_loader):\n",
    "        outputs = cnn3(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f'模型在val集上的准确率：{accuracy * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:32:05.430355800Z",
     "start_time": "2024-01-04T15:32:05.334425500Z"
    }
   },
   "id": "e926023c2fc11a60"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
