{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-04T08:49:42.387782Z",
     "start_time": "2024-01-04T08:49:38.833072500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20230921a\\.conda\\envs\\pymarl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from CNN import CNN\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from trainDataSet import trainDataSet\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "torch.manual_seed(1)  # 使用随机化种子使神经网络的初始化每次都相同\n",
    "from testDataSet import testDataSet\n",
    "# 超参数\n",
    "EPOCH = 5 # 训练整批数据的次数\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001  # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "correct_file_label=[]\n",
    "def delete_error_img():\n",
    "    \"\"\"\n",
    "    由于train数据集中图片噪音过多，现用官方数据训练完成的模型对一些标签错误的图片进行筛选，模型准确率已经在98%左右；\n",
    "    如果模型预测的标签与实际标签不一致，则删除该图片\n",
    "    \"\"\"\n",
    "\n",
    "    root_dir = './processed_data/train'  # 替换为数据集根目录路径\n",
    "    dataset = trainDataSet(root_dir)\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    # 加载模型\n",
    "\n",
    "    cnn = CNN()\n",
    "    # 如果模型已经训练过，确保加载模型权重\n",
    "    #该模型是用mnist官方数据集训练完成的\n",
    "    cnn.load_state_dict(torch.load('cnn2.pkl'))\n",
    "    # 将模型设置为评估模式\n",
    "    cnn.eval()\n",
    "    correct_index=[]#需要被删除的下标\n",
    "    for index, (data, label) in enumerate(tqdm(train_loader)):\n",
    "        # 模型预测\n",
    "        output = cnn(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        # 检查预测是否正确\n",
    "        batch_start_index = index * train_loader.batch_size\n",
    "        for idx, pred in enumerate(predicted):\n",
    "            absolute_idx = batch_start_index + idx  # 计算在整个数据集中的索引\n",
    "            if pred.item() == label[idx].item():\n",
    "                correct_index.append(absolute_idx)\n",
    "                correct_file_label.append(pred.item())\n",
    "              \n",
    "    print(\"删除噪音数据中........大概2min\")\n",
    "    dataset.get_correct_data(correct_index)\n",
    "    dataset.set_labels(correct_file_label)\n",
    "    # 创建一个新的 dataset\n",
    "    new_dataset = train_loader.dataset\n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "def train_for_new_model(train_loader):\n",
    "    \"\"\"\n",
    "    dataloader是要训练的数据集\n",
    "    通过cnn训练已经清理完成的数据集，来得到一个针对于该数据集的新模型。\n",
    "    \"\"\"\n",
    "    cnn2 = CNN()\n",
    "    # 优化器选择Adam\n",
    "    optimizer = torch.optim.Adam(cnn2.parameters(), lr=LR)\n",
    "    # 损失函数\n",
    "    loss_func = nn.CrossEntropyLoss()  # 目标标签是one-hotted\n",
    "    # 开始训练\n",
    "    for epoch in range(EPOCH):\n",
    "        for step, (b_x, b_y) in enumerate(tqdm(train_loader)):  # 分配batch data\n",
    "            output = cnn2(b_x)  # 先将数据放到cnn中计算output\n",
    "            loss = loss_func(output, b_y)  # 输出和真实标签的loss，二者位置不可颠倒\n",
    "            optimizer.zero_grad()  # 清除之前学到的梯度的参数\n",
    "            loss.backward()  # 反向传播，计算梯度\n",
    "            optimizer.step()  # 应用梯度\n",
    "    torch.save(cnn2.state_dict(), 'cnnLast.pkl')#保存新模型"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T09:49:10.924373400Z",
     "start_time": "2024-01-04T09:49:10.906370800Z"
    }
   },
   "id": "f15a81212f41da47"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除噪音数据中........大概2min\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[59], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m newDataSet\u001B[38;5;241m=\u001B[39m\u001B[43mdelete_error_img\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[58], line 34\u001B[0m, in \u001B[0;36mdelete_error_img\u001B[1;34m()\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m删除噪音数据中........大概2min\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     33\u001B[0m dataset\u001B[38;5;241m.\u001B[39mget_correct_data(correct_index)\n\u001B[1;32m---> 34\u001B[0m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_labels\u001B[49m(correct_file_label)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# 创建一个新的 dataset\u001B[39;00m\n\u001B[0;32m     36\u001B[0m new_dataset \u001B[38;5;241m=\u001B[39m train_loader\u001B[38;5;241m.\u001B[39mdataset\n",
      "File \u001B[1;32m~\\.conda\\envs\\pymarl\\lib\\site-packages\\torch\\utils\\data\\dataset.py:83\u001B[0m, in \u001B[0;36mDataset.__getattr__\u001B[1;34m(self, attribute_name)\u001B[0m\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m function\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 83\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "newDataSet=delete_error_img()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T09:50:10.248730600Z",
     "start_time": "2024-01-04T09:49:16.139684900Z"
    }
   },
   "id": "b50bc1d27b842792"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "44702"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newDataSet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T09:37:49.794897400Z",
     "start_time": "2024-01-04T09:37:49.779893800Z"
    }
   },
   "id": "a407831a5db90dfc"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[62], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mnewDataSet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_labels\u001B[49m(correct_file_label)\n\u001B[0;32m      2\u001B[0m filename\u001B[38;5;241m=\u001B[39mnewDataSet\u001B[38;5;241m.\u001B[39mdata_files\n\u001B[0;32m      3\u001B[0m labels\u001B[38;5;241m=\u001B[39mnewDataSet\u001B[38;5;241m.\u001B[39mlabels\n",
      "File \u001B[1;32m~\\.conda\\envs\\pymarl\\lib\\site-packages\\torch\\utils\\data\\dataset.py:83\u001B[0m, in \u001B[0;36mDataset.__getattr__\u001B[1;34m(self, attribute_name)\u001B[0m\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m function\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 83\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "newDataSet.set_labels(correct_file_label)\n",
    "filename=newDataSet.data_files\n",
    "labels=newDataSet.labels\n",
    "correctFile=pd.DataFrame({\"name\":filename,\"labels\":labels,\"correct_labels\":correct_file_label}).to_csv(\"1.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T09:50:44.602991Z",
     "start_time": "2024-01-04T09:50:44.563964600Z"
    }
   },
   "id": "6c71f5aef285e9a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# root_dir = './processed_data/train'  # 替换为数据集根目录路径\n",
    "# dataset1 = trainDataSet(root_dir)\n",
    "# train_loader = DataLoader(dataset1, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# print(len(dataset1))\n",
    "# old_filename=train_loader.dataset.data_files\n",
    "# old_labels=train_loader.dataset.labels\n",
    "# pd.DataFrame({\"name\":old_filename,\"labels\":old_labels}).to_csv(\"hahah1.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T08:50:37.538320300Z",
     "start_time": "2024-01-04T08:50:37.534320900Z"
    }
   },
   "id": "5187420d533cfb6a"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "CNN(\n  (conv1): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Dropout(p=0.5, inplace=False)\n  )\n  (out): Linear(in_features=1568, out_features=10, bias=True)\n)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "# 如果模型已经训练过，确保加载模型权重\n",
    "#该模型是用mnist官方数据集训练完成的\n",
    "cnn.load_state_dict(torch.load('cnn2.pkl'))\n",
    "# 将模型设置为评估模式\n",
    "cnn.eval()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T09:38:27.549740800Z",
     "start_time": "2024-01-04T09:38:27.525167200Z"
    }
   },
   "id": "44ef01afb92dc761"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#看留下的数据集在原来的模型中准确率如何\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "pp=[]\n",
    "la=[]\n",
    "newloader=DataLoader(newDataSet,batch_size=BATCH_SIZE,shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for data, labels in tqdm(newloader):\n",
    "        outputs = cnn(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        pp.append(predicted.cpu().numpy())\n",
    "        la.append(labels.cpu().numpy())\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f'模型在数据集上的准确率：{accuracy * 100:.2f}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-04T09:50:10.249730Z"
    }
   },
   "id": "3655c4e22d9a4706"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# correctFile['pred_lab']=pp\n",
    "flattened_array1 = np.concatenate(pp)\n",
    "flattened_array2 = np.concatenate(la)\n",
    "# correctFile['correct_label']=flattened_array\n",
    "pd.DataFrame(flattened_array1).to_csv(\"2.csv\",index=False)\n",
    "pd.DataFrame(flattened_array2).to_csv(\"3.csv\",index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T09:50:31.625211600Z",
     "start_time": "2024-01-04T09:50:31.569205900Z"
    }
   },
   "id": "8148493c886bab47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in tqdm(train_loader):\n",
    "        outputs = cnn(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f'模型在数据集上的准确率：{accuracy * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-04T08:50:37.539323800Z"
    }
   },
   "id": "13934bbd17b05bc3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
